{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c51dfac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q streamlit requests gTTS pillow python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, uuid, base64, json\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import requests\n",
    "from gtts import gTTS\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env if present\n",
    "load_dotenv()\n",
    "\n",
    "# ---- CONFIG ----\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")  # safer: set in Streamlit Cloud secrets\n",
    "MODEL_TEXT = \"HuggingFaceH4/zephyr-7b-beta\"  # text generation model\n",
    "MODEL_IMAGE = \"stabilityai/stable-diffusion-xl-base-1.0\"      # stil need fixing optional image model\n",
    "N_FOLLOWUPS = 3\n",
    "OUTPUT_DIR = Path(\"outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8c6ec6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLLOWUP_PROMPT = \"\"\"You are an insightful, concise dream analyst.\n",
    "Ask {n} SHORT follow-up questions to better understand this dream. \n",
    "No preamble, just numbered questions on separate lines.\n",
    "\n",
    "Dream:\n",
    "\\\"\\\"\\\"{dream}\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "POEM_PROMPT = \"\"\"You are a poetic interpreter of dreams.\n",
    "Using the dream and answers, write a flowing poetic prose (150‚Äì250 words). \n",
    "No bullet points, no headers‚Äîjust a single evocative passage.\n",
    "\n",
    "Context:\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "IMAGE_PROMPT_TMPL = \"\"\"Write a single <120 word prompt for an image generator capturing the symbolism, mood and key elements of this dream context:\n",
    "\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fc140b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4 ‚Äî Hugging Face helpers (patched with wait_for_model + clear errors) =====\n",
    "import requests, base64\n",
    "from typing import List\n",
    "\n",
    "def _raise_for_hf_error(resp: requests.Response):\n",
    "    \"\"\"Give clearer messages than requests.raise_for_status().\"\"\"\n",
    "    if resp.status_code == 401:\n",
    "        raise RuntimeError(\"HF 401 Unauthorized: Token missing/invalid. Check HF_TOKEN.\")\n",
    "    if resp.status_code == 403:\n",
    "        raise RuntimeError(\"HF 403 Forbidden: You don't have access to this model.\")\n",
    "    if resp.status_code == 429:\n",
    "        raise RuntimeError(\"HF 429 Rate limit: Too many requests. Slow down or upgrade.\")\n",
    "    if resp.status_code == 503:\n",
    "        # common when model is loading; wait_for_model=true should help, but still‚Ä¶\n",
    "        raise RuntimeError(\"HF 503: Model loading or unavailable. Try again in a few seconds.\")\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if isinstance(data, dict) and \"error\" in data:\n",
    "        raise RuntimeError(f'HF Error: {data[\"error\"]}')\n",
    "    return data\n",
    "\n",
    "def hf_text(prompt: str,\n",
    "            model: str = MODEL_TEXT,\n",
    "            max_new_tokens: int = 300,\n",
    "            temperature: float = 0.8) -> str:\n",
    "    url = f\"https://api-inference.huggingface.co/models/{model}?wait_for_model=true\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    }\n",
    "    r = requests.post(url, headers=headers, json=payload, timeout=180)\n",
    "    data = _raise_for_hf_error(r)\n",
    "    # HF returns a list of dicts: [{'generated_text': '...'}]\n",
    "    return data[0][\"generated_text\"].strip()\n",
    "\n",
    "def clean_questions(raw: str, n: int) -> List[str]:\n",
    "    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]\n",
    "    qs = []\n",
    "    for ln in lines:\n",
    "        ln = ln.lstrip(\"0123456789).:- \").strip()\n",
    "        if ln:\n",
    "            qs.append(ln)\n",
    "    return qs[:n]\n",
    "\n",
    "def gen_followups(dream: str, n: int = N_FOLLOWUPS) -> List[str]:\n",
    "    raw = hf_text(FOLLOWUP_PROMPT.format(dream=dream, n=n))\n",
    "    return clean_questions(raw, n)\n",
    "\n",
    "def gen_poem(context: str) -> str:\n",
    "    return hf_text(POEM_PROMPT.format(context=context),\n",
    "                   temperature=0.85,\n",
    "                   max_new_tokens=400)\n",
    "\n",
    "def gen_image(context: str) -> bytes:\n",
    "    # 1) Ask text model to craft a concise SDXL prompt\n",
    "    img_prompt = hf_text(IMAGE_PROMPT_TMPL.format(context=context),\n",
    "                         temperature=0.6,\n",
    "                         max_new_tokens=120)\n",
    "    # 2) Call SDXL (binary image response)\n",
    "    url = f\"https://api-inference.huggingface.co/models/{MODEL_IMAGE}?wait_for_model=true\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "    payload = {\"inputs\": img_prompt}\n",
    "    r = requests.post(url, headers=headers, json=payload, timeout=240)\n",
    "    _raise_for_hf_error(r)  # will raise if JSON w/ error\n",
    "    return r.content  # raw PNG/JPEG bytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bcff5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tts_mp3(text: str) -> Path:\n",
    "    path = OUTPUT_DIR / f\"tts_{uuid.uuid4().hex}.mp3\"\n",
    "    gTTS(text).save(str(path))\n",
    "    return path\n",
    "\n",
    "def save_image_bytes(img_bytes: bytes) -> Path:\n",
    "    path = OUTPUT_DIR / f\"img_{uuid.uuid4().hex}.png\"\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(img_bytes)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f88dd8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cli():\n",
    "    dream = input(\"Describe your dream:\\n> \")\n",
    "    qs = gen_followups(dream)\n",
    "    print(\"\\nFollow-up questions:\")\n",
    "    answers = []\n",
    "    for i, q in enumerate(qs, 1):\n",
    "        ans = input(f\"{i}) {q}\\n> \")\n",
    "        answers.append(ans)\n",
    "\n",
    "    context = f\"Dream: {dream}\\nAnswers:\\n\" + \"\\n\".join(f\"- {a}\" for a in answers)\n",
    "\n",
    "    mode = input(\"\\nOutput type? [text/image] default=text: \").strip().lower() or \"text\"\n",
    "\n",
    "    if mode == \"image\":\n",
    "        prose = gen_poem(context)\n",
    "        img_bytes = gen_image(context)\n",
    "        img_path = save_image_bytes(img_bytes)\n",
    "        print(\"\\n--- PROSE ---\\n\")\n",
    "        print(prose)\n",
    "        display(Image.open(img_path))\n",
    "    else:\n",
    "        prose = gen_poem(context)\n",
    "        print(\"\\n--- PROSE ---\\n\")\n",
    "        print(prose)\n",
    "\n",
    "    if input(\"\\nMake TTS? [y/N]: \").strip().lower() == \"y\":\n",
    "        audio_path = make_tts_mp3(prose)\n",
    "        print(f\"Saved audio to {audio_path}\")\n",
    "\n",
    "# Uncomment to test interactively:\n",
    "# run_cli()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 12:04:38.969 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.971 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.971 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.972 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.972 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.973 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.975 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.978 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.979 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.979 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.980 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.981 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.982 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.982 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-07-25 12:04:38.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "#%%writefile streamlit_app.py\n",
    "import os, uuid, base64\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from gtts import gTTS\n",
    "from PIL import Image\n",
    "import streamlit as st\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
    "MODEL_TEXT  = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "MODEL_IMAGE = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "N_FOLLOWUPS = 3\n",
    "OUTPUT_DIR = Path(\"outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "FOLLOWUP_PROMPT = \"\"\"You are an insightful, concise dream analyst.\n",
    "Ask {n} SHORT follow-up questions to better understand this dream.\n",
    "No preamble, just numbered questions on separate lines.\n",
    "\n",
    "Dream:\n",
    "\\\"\\\"\\\"{dream}\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "POEM_PROMPT = \"\"\"You are a poetic interpreter of dreams.\n",
    "Using the dream and answers, write a flowing poetic prose (150‚Äì250 words). \n",
    "No bullet points or headers‚Äîjust a single evocative passage.\n",
    "\n",
    "Context:\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "IMAGE_PROMPT_TMPL = \"\"\"Write a single <120 word prompt for an image generator capturing the symbolism, mood and key elements of this dream context:\n",
    "\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up OpenAI-style Hugging Face router client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "def hf_text(prompt: str,\n",
    "            model: str = \"HuggingFaceH4/zephyr-7b-beta:featherless-ai\",\n",
    "            max_new_tokens: int = 300,\n",
    "            temperature: float = 0.8) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def clean_questions(raw: str, n: int):\n",
    "    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]\n",
    "    qs = []\n",
    "    for ln in lines:\n",
    "        ln = ln.lstrip(\"0123456789).:- \").strip()\n",
    "        if ln:\n",
    "            qs.append(ln)\n",
    "    return qs[:n]\n",
    "\n",
    "def gen_followups(dream: str, n: int = N_FOLLOWUPS):\n",
    "    raw = hf_text(FOLLOWUP_PROMPT.format(dream=dream, n=n))\n",
    "    return clean_questions(raw, n)\n",
    "\n",
    "def gen_poem(context: str):\n",
    "    return hf_text(POEM_PROMPT.format(context=context), temperature=0.85, max_new_tokens=400)\n",
    "\n",
    "def gen_image(context: str):\n",
    "    img_prompt = hf_text(IMAGE_PROMPT_TMPL.format(context=context), temperature=0.6, max_new_tokens=120)\n",
    "    url = f\"https://api-inference.huggingface.co/models/{MODEL_IMAGE}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "    payload = {\"inputs\": img_prompt}\n",
    "    r = requests.post(url, headers=headers, json=payload, timeout=180)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def save_mp3(text: str):\n",
    "    path = OUTPUT_DIR / f\"tts_{uuid.uuid4().hex}.mp3\"\n",
    "    gTTS(text).save(str(path))\n",
    "    return path\n",
    "\n",
    "# ------------- UI -------------\n",
    "st.set_page_config(page_title=\"DreamSense\", page_icon=\"üåô\", layout=\"centered\")\n",
    "st.title(\"üåô DreamSense\")\n",
    "st.caption(\"Type your dream. We'll ask a few questions, then craft something beautiful.\")\n",
    "\n",
    "dream = st.text_area(\"üìù Describe your dream\", height=200)\n",
    "\n",
    "if dream and st.button(\"Get follow-up questions\"):\n",
    "    st.session_state.questions = gen_followups(dream)\n",
    "    st.session_state.answers   = [\"\"] * len(st.session_state.questions)\n",
    "\n",
    "if \"questions\" in st.session_state:\n",
    "    st.subheader(\"üîç Follow-up Questions\")\n",
    "    for i, q in enumerate(st.session_state.questions):\n",
    "        st.session_state.answers[i] = st.text_input(q, key=f\"fq_{i}\")\n",
    "\n",
    "    output_type = st.radio(\"Output type\", [\"Poem / Prose (text)\", \"Image + Poem\"], index=0)\n",
    "    do_tts = st.checkbox(\"üîä Read it aloud\")\n",
    "\n",
    "    if all(st.session_state.answers) and st.button(\"Generate ‚ú®\"):\n",
    "        context = \"Dream: \" + dream + \"\\nAnswers:\\n\" + \"\\n\".join(f\"- {a}\" for a in st.session_state.answers)\n",
    "\n",
    "        if output_type.startswith(\"Image\"):\n",
    "            prose = gen_poem(context)\n",
    "            img_bytes = gen_image(context)\n",
    "            st.markdown(\"### üñºÔ∏è Image\")\n",
    "            st.image(Image.open(BytesIO(img_bytes)))\n",
    "            st.markdown(\"### üìù Poetic Interpretation\")\n",
    "            st.write(prose)\n",
    "        else:\n",
    "            prose = gen_poem(context)\n",
    "            st.markdown(\"### üìù Poetic Interpretation\")\n",
    "            st.write(prose)\n",
    "\n",
    "        if do_tts:\n",
    "            mp3_path = save_mp3(prose)\n",
    "            st.audio(str(mp3_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "add32fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8520\n",
      "  Network URL: http://192.168.0.32:8520\n",
      "  External URL: http://92.233.134.66:8520\n",
      "\n",
      "  For better performance, install the Watchdog module:\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \n",
      "Streamlit is running. Copy the URL shown above or in the terminal output.\n"
     ]
    }
   ],
   "source": [
    "# ONLY run if you want to preview the web app right from the notebook.\n",
    "# Stop with the stop button in Jupyter or Ctrl+C in terminal.\n",
    "import subprocess, sys, time\n",
    "\n",
    "proc = subprocess.Popen([sys.executable, \"-m\", \"streamlit\", \"run\", \"streamlit_app.py\", \"--server.headless\", \"true\"])\n",
    "time.sleep(3)\n",
    "print(\"Streamlit is running. Copy the URL shown above or in the terminal output.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba779ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ee498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 (TTS)",
   "language": "python",
   "name": "py310tts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
